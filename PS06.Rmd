---
title: "STAT/MATH 495: Problem Set 06"
author: "Sarah Teichman"
date: "2017-10-17"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    collapsed: false
    smooth_scroll: false
    df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, fig.width=8, fig.height=4.5, message=FALSE, warning = FALSE
  )
set.seed(76)

# Load packages
library(tidyverse)
library(broom)
library(knitr)
```





# Collaboration

Please indicate who you collaborated with on this assignment: Jonathan





# Setup

Define truth, which again we know for the purposes of this assignment, but in
practice we won't:

* the true function f(x) i.e. the signal
* the true epsilon i.e. the noise, which in this case is Normal$(0, sd=\sigma)$.
Hence the standard deviation $\sigma$ determines the amount of noise.

```{r}
f <- function(x) {
  x^2
}
sigma <- 0.3
```

This is the target point we'll be trying to predict: $(0.95, f(0.95)) = (0.95, 0.95^2) = (0.95, 0.9025)$, Thus, the test set is just `x=0.95`

```{r}
x0 <- 0.95
test_set <- data_frame(x=x0)
```

This function generates a random sample of size $n$; think of this as a "get new
data" function. Random in terms of both:

* (New) the predictor x (uniform on [0,1])
* the amount of noise $\epsilon$

```{r}
generate_sample <- function(f, n, sigma) {
  sample <- data_frame(
    x = runif(n = n, min = 0, max = 1),
    f_x = f(x),
    epsilon = rnorm(n = n, mean = 0, sd = sigma),
    y = f_x + epsilon
  )
  # Recall: We don't observe f(x) and epsilon, just (x, y)
  sample <- sample %>% 
    select(x, y)
  
  return(sample)
}
```

Define

* The number $n$ of observations $(x_i, y_i)$ in each sample. In the handout,
$n=100$ to keep plots uncrowded. Here we boost to $n=500$
* Number of samples of size $n$ to consider

```{r}
n <- 500
n_sample <- 10000
```


# Computation

```{r}
do_stuff <- function(df,f,n,sigma) {
  samp <- generate_sample(f,n,sigma)
  mod <- smooth.spline(samp$x,samp$y,df=df)
  pred <- predict(mod,0.95)[[2]]
  return(pred)
}
df2 <- numeric(n_sample)
df99 <- numeric(n_sample)
pred_df2 <- replicate(n_sample,do_stuff(df=2,f=f,n=n,sigma=sigma))
pred_df99 <- replicate(n_sample,do_stuff(df=99,f=f,n=n,sigma=sigma))
pred_df2 <- data.frame(pred_df2)
pred_df99 <- data.frame(pred_df99)
f_x <- rep(.95^2, n_sample)
epsilon <- rnorm(n_sample,0,sigma)
y <- f_x + epsilon
df_2 <- pred_df2 %>%
  summarise(mse = mean((pred_df2-y)^2),
            bias_sq = mean((pred_df2)-f_x)^2,
            variance = mean((pred_df2-mean(pred_df2))^2),
            irreducible = (sigma^2),
            sum = bias_sq+variance+irreducible)
df_99 <- pred_df99 %>%
  summarise(mse = mean((pred_df99-y)^2),
            bias_sq = mean((pred_df99)-f_x)^2,
            variance = mean((pred_df99-mean(pred_df99))^2),
            irreducible = (sigma^2),
            sum = bias_sq+variance+irreducible)
df_all <- rbind(df_2,df_99)
rownames(df_all) <- c('lm','spline df=99')
```

# Tables

```{r}
df_all %>%
  knitr::kable(digits=4)
```

# Analysis

**Questions**:

1. Based on the topics covered in Lec 2.7, name one possible "sanity check" for your results. Name another if you can.
2. In **two** sentences or less, give a rough sketch of what the procedure would
be to get the breakdown of $$\mbox{MSE}\left[\widehat{f}(x)\right]$$ for *all*
$x$ in this example, and not just for $$\mbox{MSE}\left[\widehat{f}(x_0)\right]
= \mbox{MSE}\left[\widehat{f}(0.95)\right]$$.
3. Which of the two models would you choose for predicting the point of interest and why?

**Answers**:

1. One sanity check could be to plot the two spline models for a random sample of 100 of the 10000 times I ran through the process of generating predictions, and to plot the true underlyng curve. For df=2, there would be low variance but the points would not line up exactly with the true curve, and for df=99, there would be much higher variance but the points would be clustered around $0.95^2$. Another much more expensive sanity check would be to redo the entire process with a different value of sigma. If the sample size is large enough, the bias and variance should stay relatively constant (although the estimates would be less precise) but the mse and irreducible error would increase. This would show which parts of the process are random/depend on the exact points and which parts are more systematic.

2. I would calculate each component by taking the mean of the error component estimate for each the 10,000 samples. The MSE estimates would come from the differences between the 500 y values for the data points, and their corresponding predictions from the model, and the bias and variance would be similar but comparing the distributions of the predicted values over the 10,000 samples.

3. I would use the lm model because it has a lower MSE. Even though it has higher bias, it has lower variance and that low variance keeps the prediction from being too far away from the true value. However, if I were trying to find an optimal model I would try an intermediate number of degrees of freedom in order to reduce the the sum of the squared bias and variance as much as possible. 
